{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "train_x_mean= tensor([10.0400,  5.0200])\n",
      "train_x_std= tensor([5.7562, 2.8781])\n",
      "train_y_mean= 1798.107421875\n",
      "train_y_std= 1174.586181640625\n",
      "test_x_mean= tensor([-2.4414e-08,  1.6479e-07])\n",
      "test_x_std= tensor([1.0091, 1.0091])\n",
      "test_y_mean= 0.020148921757936478\n",
      "test_y_std= 1.0429712533950806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chexuanyou/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_581jc0ddx6/croot/pytorch-select_1730848714937/work/aten/src/ATen/native/TensorShape.cpp:3588.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/200 Done, Total Loss: 0.023781247008753944\n",
      "Epoch 1/200 Done, Total Loss: 0.006826739700343327\n",
      "Epoch 2/200 Done, Total Loss: 0.00499798873205409\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 267\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m## And return the model in case we want to use it for other tasks\u001b[39;00m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m--> 267\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 243\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, learn_rate, EPOCHS)\u001b[0m\n\u001b[1;32m    240\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out\u001b[38;5;241m.\u001b[39msqueeze(), label\u001b[38;5;241m.\u001b[39mto(device)) \u001b[38;5;66;03m# Now, calculate the loss value\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m## Here, the whole magic happens...\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m## For the plot at the end, save the loss values\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import matplotlib for plotting purposes\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"text.usetex\"] = True\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sometimes, we need Numpy, but wherever possible, we prefer torch.\n",
    "import numpy as np\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Float (32 bit) or Double (64 bit) precision? Choose!\n",
    "torch.set_default_dtype(torch.float32)#64)\n",
    "torch.set_num_threads(4) # Use _maximally_ 4 CPU cores\n",
    "\n",
    "# Material parameters\n",
    "mu = 384.614  # Pa\n",
    "lamda = 576.923  # Pa \n",
    "\n",
    "# Choose a device for major calculations (if there is a special GPU card, you usually want that).\n",
    "# My GPU is not very performant for double precision, so I stay on CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "device = torch.device(device)\n",
    "\n",
    "# File path for saving the trained NN later.\n",
    "# If only providing the file name (like here), the file will be generated in the same folder as the Python script\n",
    "model_file = \"CANN_small.torch\"\n",
    "\n",
    "## These parameters are usual hyperparameters of the NN and its training.\n",
    "batch_size = 1024 # How many samples shall be presented to the NN, before running another optimizer step?\n",
    "hidden_dim = 128 # How many neurons shall there be in the hidden layer(s)?\n",
    "input_dim = 2 # Input dimension of the NN (i.e. how many neurons are in the input layer?)\n",
    "output_dim = 1 # Some for output\n",
    "\n",
    "epochs = 200 # Number of training iterations to be performed\n",
    "lr = 1e-3 # Which learning rate is passed to the training algorithm?\n",
    "\n",
    "## Choose a criterion to evaluate the results. Here, we choose Mean Square error.\n",
    "## The term \"loss\" means about the same as \"remaining error\" or \"residual\".\n",
    "criterion = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "## Here, we create the training data. In this example, we draw samples within the sampling interval and then pass it to\n",
    "## e.g. the sin function (choose other functions to experiment with this script)\n",
    "min_I1 = 0.07999999999999996\n",
    "max_I1 = 20.0\n",
    "\n",
    "min_J = 0.03999999999999998\n",
    "max_J = 10.0\n",
    "\n",
    "# create uniformly distributed samples for I1 and J\n",
    "# define the number of samples\n",
    "n_samples = 1000\n",
    "\n",
    "# create uniformly distributed samples for I1 and J\n",
    "I1_uniform_samples = torch.linspace(min_I1, max_I1, n_samples)\n",
    "J_uniform_samples = torch.linspace(min_J, max_J, n_samples)\n",
    "\n",
    "# create all possible combinations of I1 and J\n",
    "I1_uniform, J_uniform = torch.meshgrid(I1_uniform_samples, J_uniform_samples)\n",
    "\n",
    "# flatten the tensors\n",
    "I1_uniform = I1_uniform.flatten()\n",
    "J_uniform = J_uniform.flatten()\n",
    "\n",
    "# calculate the energy density for each combination\n",
    "psi_uniform = lamda / 2 * (torch.log(J_uniform))**2 - mu * torch.log(J_uniform) + mu / 2 * (I1_uniform - 2)\n",
    "\n",
    "# Input is I1 and J, output is psi, create train dataset\n",
    "train_x_ori = torch.stack((I1_uniform, J_uniform), dim=1)\n",
    "train_y_ori = psi_uniform\n",
    "\n",
    "# save the mean and standard deviation of the training data for later use\n",
    "train_x_mean = train_x_ori.mean(dim=0)\n",
    "train_x_std = train_x_ori.std(dim=0)\n",
    "train_y_mean = train_y_ori.mean(dim=0)\n",
    "train_y_std = train_y_ori.std(dim=0)\n",
    "\n",
    "train_x = (train_x_ori - train_x_mean) / train_x_std\n",
    "train_y = (train_y_ori - train_y_mean) / train_y_std\n",
    "\n",
    "# create test data with the same min und max values but different number of samples\n",
    "n_samples = 100\n",
    "\n",
    "# create uniformly distributed samples for I1 and J\n",
    "I1_uniform_samples = torch.linspace(min_I1, max_I1, n_samples)\n",
    "J_uniform_samples = torch.linspace(min_J, max_J, n_samples)\n",
    "\n",
    "# create all possible combinations of I1 and J\n",
    "I1_uniform, J_uniform = torch.meshgrid(I1_uniform_samples, J_uniform_samples)\n",
    "\n",
    "# flatten the tensors\n",
    "I1_uniform = I1_uniform.flatten()\n",
    "J_uniform = J_uniform.flatten()\n",
    "\n",
    "# standard deviation and mean of the test data\n",
    "\n",
    "\n",
    "# calculate the energy density for each combination\n",
    "psi_uniform = lamda / 2 * (torch.log(J_uniform))**2 - mu * torch.log(J_uniform) + mu / 2 * (I1_uniform - 2)\n",
    "\n",
    "# Input is I1 and J, output is psi, create test dataset\n",
    "test_x = torch.stack((I1_uniform, J_uniform), dim=1)\n",
    "test_y = psi_uniform\n",
    "\n",
    "test_x = (test_x - train_x_mean) / train_x_std\n",
    "test_y = (test_y - train_y_mean) / train_y_std\n",
    "\n",
    "\n",
    "# print all the std and mean values\n",
    "print(f\"train_x_mean= {train_x_mean}\")\n",
    "print(f\"train_x_std= {train_x_std}\")\n",
    "print(f\"train_y_mean= {train_y_mean}\")\n",
    "print(f\"train_y_std= {train_y_std}\")\n",
    "print(f\"test_x_mean= {test_x.mean(dim=0)}\")\n",
    "print(f\"test_x_std= {test_x.std(dim=0)}\")\n",
    "print(f\"test_y_mean= {test_y.mean(dim=0)}\")\n",
    "print(f\"test_y_std= {test_y.std(dim=0)}\")\n",
    "\n",
    "\n",
    "## We want a DataLoader to handle batching and shuffling of the training data for us.\n",
    "## The DataLoader needs a TensorDataset, hence we create one from the Training data.\n",
    "train_data = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "\n",
    "## This class creates the actual Neural Network.\n",
    "class MLNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLNet, self).__init__()\n",
    "\n",
    "        # First layer: half linear, half quadratic\n",
    "        self.layer1_linear = nn.Linear(input_dim, hidden_dim // 2)\n",
    "        self.layer1_quadratic = nn.Linear(input_dim, hidden_dim // 2)\n",
    "\n",
    "        # Second layer: half exponential, half linear\n",
    "        self.layer2_linear = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.layer2_exponential = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "\n",
    "        # Final layer to combine outputs\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First layer transformations\n",
    "        linear_out = self.layer1_linear(x)\n",
    "        quadratic_out = torch.pow(self.layer1_quadratic(x), 2)\n",
    "        combined1 = torch.cat((linear_out, quadratic_out), dim=1)\n",
    "\n",
    "        # Second layer transformations\n",
    "        linear_out2 = self.layer2_linear(combined1)\n",
    "        exp_out = torch.exp(self.layer2_exponential(combined1))\n",
    "        combined2 = torch.cat((linear_out2, exp_out), dim=1)\n",
    "\n",
    "        # Final output layer\n",
    "        output = self.output_layer(combined2)\n",
    "        return output\n",
    "\n",
    "## This function performs a test run with the NN.\n",
    "## It takes the NN model and the test data, passes the test inputs through the network and\n",
    "## compares it with the target values = test outputs = targets\n",
    "## and based on that calculates the loss value\n",
    "def evaluate(model, test_x, test_y):\n",
    "    ## For testing, we don't need the autograd feature/ protocol of all calculation steps\n",
    "    ## So, save some time and disable grad tracking.\n",
    "    with torch.no_grad():\n",
    "        model.eval() ## Set the NN model into evaluation mode\n",
    "        outputs = [] ## Create empty lists to store the results\n",
    "        targets = []\n",
    "        testlosses = []\n",
    "\n",
    "        out = model(test_x.to(device)) ## Call the model, i.e. perform the actual inference\n",
    "\n",
    "        ## Move the output quantities to the CPU, detach them from the tensor operation book-keeping and convert them to numpy arrays/ vectors.\n",
    "        ## This is all necessary for plotting\n",
    "        outputs.append(out.cpu().detach().numpy())\n",
    "        targets.append(test_y.cpu().detach().numpy())\n",
    "        testlosses.append(criterion(out.squeeze(), test_y.to(device)).item())\n",
    "\n",
    "    ## Now return that in form of a triple of variables\n",
    "    return outputs, targets, testlosses\n",
    "\n",
    "## This calls the evaluate function and takes care of the plotting.\n",
    "def eval_and_plot(model):\n",
    "        ## matplotlib tries to be similar to the plot functions of matlab  (admittedly, the commands have to begin with \"plt.\", but the rest\n",
    "        ## is quite similar.)\n",
    "        #plt.subplot(1, 1, 1)\n",
    "\n",
    "        ## Call the network on the test data\n",
    "        net_outputs_test, targets_test, testlosses = evaluate(model, test_x, test_y)\n",
    "        # Plot the targets first in blue (which means to plot the actual function over the whole test interval)\n",
    "        #plt.scatter()#, \"-x\", color=\"b\", label=\"Target\")\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(test_x[:,0], test_x[:,1], targets_test, marker='x', c='skyblue', s=60)\n",
    "\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"y\")\n",
    "        plt.legend()\n",
    "\n",
    "        ## Call the network on the training data\n",
    "        net_outputs_train, targets_train, testlosses = evaluate(model, train_x, train_y)\n",
    "        ## First, plot the targets in red, i.e. plot the training data set\n",
    "        ax.scatter(train_x[:,0], train_x[:,1], targets_train[0], marker='^', c='red', s=60)\n",
    "\n",
    "        ## Now, plot the output of the NN on the whole test interval in green\n",
    "        ## This allows us to see how the NN performs for interpolation as well as for extrapolation\n",
    "        ax.scatter(test_x[:,0], test_x[:,1], net_outputs_test[0], marker='o', c='green', s=60)\n",
    "\n",
    "        ## All plotting is done, open the plot window\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "## That function takes care of the whole training\n",
    "def train(train_loader, learn_rate, EPOCHS):  # 10):\n",
    "    # Instantiate the NN\n",
    "    model = MLNet(input_dim, hidden_dim, output_dim)\n",
    "    model.to(device) # and move it to the \"device\" (in case we use a GPU)\n",
    "    ## Choose an optimizer.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "\n",
    "    # We want to keep track of the losses averaged over each epoch, to plot them\n",
    "    avg_losses = torch.zeros(EPOCHS)\n",
    "\n",
    "    ## In the end, epoch is just another word for \"training iteration\", so we have a simple for loop.\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train() # Set the model into train mode\n",
    "        avg_loss = 0. # initializations\n",
    "\n",
    "        ## DataLoader is iterable so that this for-loop loops over the batches of the training data set\n",
    "        ## and the DataLoader gives us readily paired combinations of training inputs and targets (which are called x and label, here).\n",
    "        for x, label in train_loader:\n",
    "            model.zero_grad() # Important: reset the gradients of the NN before passing the training inputs.\n",
    "            # Otherwise, we would accumulate the gradient information which might ruin the results\n",
    "            # or simply run into PyTorch exceptions\n",
    "\n",
    "            ## Now, we can call the model on the training inputs.\n",
    "            out = model(x.to(device))\n",
    "            loss = criterion(out.squeeze(), label.to(device)) # Now, calculate the loss value\n",
    "\n",
    "            ## Here, the whole magic happens...\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ## For the plot at the end, save the loss values\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "        print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss / len(train_loader)))\n",
    "        ## It's an average loss, so divide by the number of samples/ size of the training data set\n",
    "        avg_losses[epoch] = avg_loss / len(train_loader)\n",
    "\n",
    "        ## This plots the loss curve\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(avg_losses, \"x-\")\n",
    "    plt.title(\"Train loss (MSE, reduction=mean, averaged over epoch)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.grid(visible=True, which='both', axis='both')\n",
    "    plt.show()\n",
    "        \n",
    "    ## Now save the trained model with all its properties to the model_file\n",
    "    torch.save(model, model_file)\n",
    "    ## And return the model in case we want to use it for other tasks\n",
    "    return model\n",
    "\n",
    "model = train(train_loader, lr, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# create uniformly distributed samples for I1 and J\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m I1_uniform_samples \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mlinspace(min_I1, max_I1, n_samples)\n\u001b[1;32m     21\u001b[0m J_uniform_samples \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(min_J, max_J, n_samples)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# create all possible combinations of I1 and J\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# plot the difference between Neural Network and analytical solution\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "\n",
    "## Here, we create the test data.\n",
    "# Smaller dataset and include larger area outside the training data\n",
    "min_I1 = 0.07999999999999996\n",
    "max_I1 = 20.0\n",
    "\n",
    "min_J = 0.03999999999999998\n",
    "max_J = 10.0\n",
    "\n",
    "# create uniformly distributed samples for I1 and J\n",
    "# define the number of samples\n",
    "n_samples = 30\n",
    "\n",
    "# create uniformly distributed samples for I1 and J\n",
    "I1_uniform_samples = torch.linspace(min_I1, max_I1, n_samples)\n",
    "J_uniform_samples = torch.linspace(min_J, max_J, n_samples)\n",
    "\n",
    "# create all possible combinations of I1 and J\n",
    "I1_uniform, J_uniform = torch.meshgrid(I1_uniform_samples, J_uniform_samples)\n",
    "\n",
    "# flatten the tensors\n",
    "I1_uniform = I1_uniform.flatten()\n",
    "J_uniform = J_uniform.flatten()\n",
    "\n",
    "# calculate the energy density for each combination\n",
    "psi_uniform = lamda / 2 * (torch.log(J_uniform))**2 - mu * torch.log(J_uniform) + mu / 2 * (I1_uniform - 2)\n",
    "\n",
    "# Input is I1 and J, output is psi, create test dataset\n",
    "test_x = torch.stack((I1_uniform, J_uniform), dim=1)\n",
    "test_y = psi_uniform\n",
    "\n",
    "# normalize the test data\n",
    "test_x_norm = (test_x - train_x_mean) / train_x_std\n",
    "test_y_norm = (test_y - train_y_mean) / train_y_std\n",
    "\n",
    "# Call the network on the test data\n",
    "net_outputs_test, targets_test, testlosses = evaluate(model, test_x_norm, test_y_norm)\n",
    "\n",
    "# inverse the normalization of the net_outputs_test\n",
    "net_outputs_test = net_outputs_test * train_y_std.detach().numpy() + train_y_mean.detach().numpy()\n",
    "\n",
    "# calculate the target with the analytical solution\n",
    "psi_target = lamda / 2 * (torch.log(test_x[:, 1]))**2 - mu * torch.log(test_x[:, 1]) + mu / 2 * (test_x[:, 0] - 2)\n",
    "\n",
    "# plot the difference between Neural Network and analytical solution in subplot right and left\n",
    "fig, axs = plt.subplots(1,3, figsize=(8*2, 10))  # create figure and axes\n",
    "\n",
    "# plot the analytical solution\n",
    "sc = axs[0].scatter(test_x[:, 0].detach().numpy(), test_x[:, 1].detach().numpy(), c=psi_target.detach().numpy(), cmap='viridis', vmin=psi_target.min().item(), vmax=psi_target.max().item())\n",
    "axs[0].set_title(\"Analytical solution\")\n",
    "axs[0].set_xlabel(\"I1\")\n",
    "axs[0].set_ylabel(\"J\")\n",
    "fig.colorbar(sc, ax=axs[0], label='Helmholtz Free Energy Density $\\Psi$')\n",
    "\n",
    "# plot the Neural Network solution\n",
    "sc = axs[1].scatter(test_x[:, 0].detach().numpy(), test_x[:, 1].detach().numpy(), c=net_outputs_test, cmap='viridis', vmin=psi_target.min().item(), vmax=psi_target.max().item())\n",
    "axs[1].set_title(\"Neural Network solution\")\n",
    "axs[1].set_xlabel(\"I1\")\n",
    "axs[1].set_ylabel(\"J\")\n",
    "fig.colorbar(sc, ax=axs[1], label='Helmholtz Free Energy Density $\\Psi$')\n",
    "\n",
    "# plot the difference between Neural Network and analytical solution\n",
    "# color is absolute difference\n",
    "dif = np.abs(net_outputs_test.flatten() - psi_target.detach().numpy())\n",
    "sc = axs[2].scatter(test_x[:, 0].detach().numpy(), test_x[:, 1].detach().numpy(), c=dif, vmax=dif.max(), vmin=dif.min(), cmap='Reds')\n",
    "axs[2].set_title(\"Difference between Neural Network and Analytical solution\")\n",
    "axs[2].set_xlabel(\"I1\")\n",
    "axs[2].set_ylabel(\"J\")\n",
    "fig.colorbar(sc, ax=axs[2], label='Absolute difference')\n",
    "\n",
    "\n",
    "plt.tight_layout()  # adjust layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch [1/200]  train_loss=0.18280  val_loss=0.07703  lr=1.00e-03\n",
      "Epoch [2/200]  train_loss=0.06774  val_loss=0.05957  lr=1.00e-03\n",
      "Epoch [3/200]  train_loss=0.05120  val_loss=0.04303  lr=1.00e-03\n",
      "Epoch [4/200]  train_loss=0.03458  val_loss=0.03098  lr=1.00e-03\n",
      "Epoch [5/200]  train_loss=0.02534  val_loss=0.02460  lr=1.00e-03\n",
      "Epoch [6/200]  train_loss=0.02190  val_loss=0.02170  lr=1.00e-03\n",
      "Epoch [7/200]  train_loss=0.02043  val_loss=0.01904  lr=1.00e-03\n",
      "Epoch [8/200]  train_loss=0.01691  val_loss=0.01732  lr=1.00e-03\n",
      "Epoch [9/200]  train_loss=0.01614  val_loss=0.02060  lr=1.00e-03\n",
      "Epoch [10/200]  train_loss=0.01647  val_loss=0.01788  lr=1.00e-03\n",
      "Epoch [11/200]  train_loss=0.01480  val_loss=0.01356  lr=1.00e-03\n",
      "Epoch [12/200]  train_loss=0.01218  val_loss=0.01148  lr=1.00e-03\n",
      "Epoch [13/200]  train_loss=0.01093  val_loss=0.01431  lr=1.00e-03\n",
      "Epoch [14/200]  train_loss=0.01137  val_loss=0.01017  lr=1.00e-03\n",
      "Epoch [15/200]  train_loss=0.01016  val_loss=0.01176  lr=1.00e-03\n",
      "Epoch [16/200]  train_loss=0.00993  val_loss=0.01051  lr=1.00e-03\n",
      "Epoch [17/200]  train_loss=0.01101  val_loss=0.01027  lr=1.00e-03\n",
      "Epoch [18/200]  train_loss=0.01091  val_loss=0.01409  lr=1.00e-03\n",
      "Epoch [19/200]  train_loss=0.01131  val_loss=0.00994  lr=1.00e-03\n",
      "Epoch [20/200]  train_loss=0.00938  val_loss=0.00913  lr=1.00e-03\n",
      "Epoch [21/200]  train_loss=0.00826  val_loss=0.00876  lr=1.00e-03\n",
      "Epoch [22/200]  train_loss=0.00825  val_loss=0.01012  lr=1.00e-03\n",
      "Epoch [23/200]  train_loss=0.00856  val_loss=0.01189  lr=1.00e-03\n",
      "Epoch [24/200]  train_loss=0.00931  val_loss=0.01070  lr=1.00e-03\n",
      "Epoch [25/200]  train_loss=0.00794  val_loss=0.00868  lr=1.00e-03\n",
      "Epoch [26/200]  train_loss=0.00760  val_loss=0.01299  lr=1.00e-03\n",
      "Epoch [27/200]  train_loss=0.00828  val_loss=0.00834  lr=1.00e-03\n",
      "Epoch [28/200]  train_loss=0.00782  val_loss=0.00833  lr=1.00e-03\n",
      "Epoch [29/200]  train_loss=0.00790  val_loss=0.00785  lr=1.00e-03\n",
      "Epoch [30/200]  train_loss=0.00790  val_loss=0.00948  lr=1.00e-03\n",
      "Epoch [31/200]  train_loss=0.00778  val_loss=0.00745  lr=1.00e-03\n",
      "Epoch [32/200]  train_loss=0.00888  val_loss=0.01303  lr=1.00e-03\n",
      "Epoch [33/200]  train_loss=0.00917  val_loss=0.00749  lr=1.00e-03\n",
      "Epoch [34/200]  train_loss=0.00745  val_loss=0.01136  lr=1.00e-03\n",
      "Epoch [35/200]  train_loss=0.00845  val_loss=0.01105  lr=1.00e-03\n",
      "Epoch [36/200]  train_loss=0.01250  val_loss=0.01241  lr=1.00e-03\n",
      "Epoch [37/200]  train_loss=0.00722  val_loss=0.00698  lr=1.00e-03\n",
      "Epoch [38/200]  train_loss=0.00659  val_loss=0.00669  lr=1.00e-03\n",
      "Epoch [39/200]  train_loss=0.00654  val_loss=0.00724  lr=1.00e-03\n",
      "Epoch [40/200]  train_loss=0.00717  val_loss=0.00760  lr=1.00e-03\n",
      "Epoch [41/200]  train_loss=0.01075  val_loss=0.01161  lr=1.00e-03\n",
      "Epoch [42/200]  train_loss=0.00774  val_loss=0.00760  lr=1.00e-03\n",
      "Epoch [43/200]  train_loss=0.00665  val_loss=0.00647  lr=1.00e-03\n",
      "Epoch [44/200]  train_loss=0.00632  val_loss=0.00784  lr=1.00e-03\n",
      "Epoch [45/200]  train_loss=0.00655  val_loss=0.00930  lr=1.00e-03\n",
      "Epoch [46/200]  train_loss=0.00627  val_loss=0.00614  lr=1.00e-03\n",
      "Epoch [47/200]  train_loss=0.00584  val_loss=0.00682  lr=1.00e-03\n",
      "Epoch [48/200]  train_loss=0.00566  val_loss=0.00688  lr=1.00e-03\n",
      "Epoch [49/200]  train_loss=0.00603  val_loss=0.00997  lr=1.00e-03\n",
      "Epoch [50/200]  train_loss=0.00688  val_loss=0.00688  lr=1.00e-04\n",
      "Epoch [51/200]  train_loss=0.00540  val_loss=0.00567  lr=1.00e-04\n",
      "Epoch [52/200]  train_loss=0.00510  val_loss=0.00555  lr=1.00e-04\n",
      "Epoch [53/200]  train_loss=0.00502  val_loss=0.00563  lr=1.00e-04\n",
      "Epoch [54/200]  train_loss=0.00503  val_loss=0.00547  lr=1.00e-04\n",
      "Epoch [55/200]  train_loss=0.00500  val_loss=0.00589  lr=1.00e-04\n",
      "Epoch [56/200]  train_loss=0.00506  val_loss=0.00557  lr=1.00e-04\n",
      "Epoch [57/200]  train_loss=0.00506  val_loss=0.00545  lr=1.00e-04\n",
      "Epoch [58/200]  train_loss=0.00495  val_loss=0.00586  lr=1.00e-04\n",
      "Epoch [59/200]  train_loss=0.00497  val_loss=0.00548  lr=1.00e-04\n",
      "Epoch [60/200]  train_loss=0.00492  val_loss=0.00553  lr=1.00e-04\n",
      "Epoch [61/200]  train_loss=0.00496  val_loss=0.00537  lr=1.00e-04\n",
      "Epoch [62/200]  train_loss=0.00494  val_loss=0.00540  lr=1.00e-04\n",
      "Epoch [63/200]  train_loss=0.00494  val_loss=0.00533  lr=1.00e-04\n",
      "Epoch [64/200]  train_loss=0.00494  val_loss=0.00535  lr=1.00e-04\n",
      "Epoch [65/200]  train_loss=0.00494  val_loss=0.00551  lr=1.00e-04\n",
      "Epoch [66/200]  train_loss=0.00488  val_loss=0.00537  lr=1.00e-04\n",
      "Epoch [67/200]  train_loss=0.00489  val_loss=0.00535  lr=1.00e-04\n",
      "Epoch [68/200]  train_loss=0.00488  val_loss=0.00532  lr=1.00e-04\n",
      "Epoch [69/200]  train_loss=0.00484  val_loss=0.00541  lr=1.00e-04\n",
      "Epoch [70/200]  train_loss=0.00492  val_loss=0.00546  lr=1.00e-04\n",
      "Epoch [71/200]  train_loss=0.00487  val_loss=0.00531  lr=1.00e-04\n",
      "Epoch [72/200]  train_loss=0.00491  val_loss=0.00537  lr=1.00e-04\n",
      "Epoch [73/200]  train_loss=0.00487  val_loss=0.00539  lr=1.00e-04\n",
      "Epoch [74/200]  train_loss=0.00481  val_loss=0.00528  lr=1.00e-04\n",
      "Epoch [75/200]  train_loss=0.00484  val_loss=0.00540  lr=1.00e-04\n",
      "Epoch [76/200]  train_loss=0.00479  val_loss=0.00526  lr=1.00e-04\n",
      "Epoch [77/200]  train_loss=0.00481  val_loss=0.00554  lr=1.00e-04\n",
      "Epoch [78/200]  train_loss=0.00480  val_loss=0.00537  lr=1.00e-04\n",
      "Epoch [79/200]  train_loss=0.00475  val_loss=0.00559  lr=1.00e-04\n",
      "Epoch [80/200]  train_loss=0.00502  val_loss=0.00533  lr=1.00e-04\n",
      "Epoch [81/200]  train_loss=0.00477  val_loss=0.00537  lr=1.00e-04\n",
      "Epoch [82/200]  train_loss=0.00480  val_loss=0.00528  lr=1.00e-04\n",
      "Epoch [83/200]  train_loss=0.00488  val_loss=0.00528  lr=1.00e-04\n",
      "Epoch [84/200]  train_loss=0.00490  val_loss=0.00521  lr=1.00e-04\n",
      "Epoch [85/200]  train_loss=0.00487  val_loss=0.00524  lr=1.00e-04\n",
      "Epoch [86/200]  train_loss=0.00473  val_loss=0.00516  lr=1.00e-04\n",
      "Epoch [87/200]  train_loss=0.00477  val_loss=0.00513  lr=1.00e-04\n",
      "Epoch [88/200]  train_loss=0.00481  val_loss=0.00536  lr=1.00e-04\n",
      "Epoch [89/200]  train_loss=0.00479  val_loss=0.00516  lr=1.00e-04\n",
      "Epoch [90/200]  train_loss=0.00488  val_loss=0.00513  lr=1.00e-04\n",
      "Epoch [91/200]  train_loss=0.00498  val_loss=0.00618  lr=1.00e-04\n",
      "Epoch [92/200]  train_loss=0.00491  val_loss=0.00518  lr=1.00e-04\n",
      "Epoch [93/200]  train_loss=0.00473  val_loss=0.00529  lr=1.00e-04\n",
      "Epoch [94/200]  train_loss=0.00464  val_loss=0.00535  lr=1.00e-04\n",
      "Epoch [95/200]  train_loss=0.00471  val_loss=0.00508  lr=1.00e-04\n",
      "Epoch [96/200]  train_loss=0.00466  val_loss=0.00512  lr=1.00e-04\n",
      "Epoch [97/200]  train_loss=0.00469  val_loss=0.00513  lr=1.00e-04\n",
      "Epoch [98/200]  train_loss=0.00468  val_loss=0.00539  lr=1.00e-04\n",
      "Epoch [99/200]  train_loss=0.00464  val_loss=0.00519  lr=1.00e-04\n",
      "Epoch [100/200]  train_loss=0.00468  val_loss=0.00511  lr=1.00e-05\n",
      "Epoch [101/200]  train_loss=0.00458  val_loss=0.00508  lr=1.00e-05\n",
      "Epoch [102/200]  train_loss=0.00456  val_loss=0.00504  lr=1.00e-05\n",
      "Epoch [103/200]  train_loss=0.00455  val_loss=0.00510  lr=1.00e-05\n",
      "Epoch [104/200]  train_loss=0.00457  val_loss=0.00507  lr=1.00e-05\n",
      "Epoch [105/200]  train_loss=0.00455  val_loss=0.00503  lr=1.00e-05\n",
      "Epoch [106/200]  train_loss=0.00456  val_loss=0.00503  lr=1.00e-05\n",
      "Epoch [107/200]  train_loss=0.00456  val_loss=0.00507  lr=1.00e-05\n",
      "Epoch [108/200]  train_loss=0.00457  val_loss=0.00509  lr=1.00e-05\n",
      "Epoch [109/200]  train_loss=0.00454  val_loss=0.00504  lr=1.00e-05\n",
      "Epoch [110/200]  train_loss=0.00456  val_loss=0.00503  lr=1.00e-05\n",
      "Epoch [111/200]  train_loss=0.00454  val_loss=0.00507  lr=1.00e-05\n",
      "Epoch [112/200]  train_loss=0.00457  val_loss=0.00510  lr=1.00e-05\n",
      "Epoch [113/200]  train_loss=0.00455  val_loss=0.00507  lr=1.00e-05\n",
      "Epoch [114/200]  train_loss=0.00455  val_loss=0.00501  lr=1.00e-05\n",
      "Epoch [115/200]  train_loss=0.00454  val_loss=0.00509  lr=1.00e-05\n",
      "Epoch [116/200]  train_loss=0.00454  val_loss=0.00502  lr=1.00e-05\n",
      "Epoch [117/200]  train_loss=0.00456  val_loss=0.00521  lr=1.00e-05\n",
      "Epoch [118/200]  train_loss=0.00458  val_loss=0.00502  lr=1.00e-05\n",
      "Epoch [119/200]  train_loss=0.00453  val_loss=0.00503  lr=1.00e-05\n",
      "Epoch [120/200]  train_loss=0.00454  val_loss=0.00502  lr=1.00e-05\n",
      "Epoch [121/200]  train_loss=0.00454  val_loss=0.00504  lr=1.00e-05\n",
      "Epoch [122/200]  train_loss=0.00454  val_loss=0.00508  lr=1.00e-05\n",
      "Epoch [123/200]  train_loss=0.00454  val_loss=0.00505  lr=1.00e-05\n",
      "Epoch [124/200]  train_loss=0.00454  val_loss=0.00503  lr=1.00e-05\n",
      "Epoch [125/200]  train_loss=0.00454  val_loss=0.00504  lr=1.00e-05\n",
      "Epoch [126/200]  train_loss=0.00453  val_loss=0.00503  lr=1.00e-05\n",
      "Epoch [127/200]  train_loss=0.00454  val_loss=0.00514  lr=1.00e-05\n",
      "Epoch [128/200]  train_loss=0.00457  val_loss=0.00500  lr=1.00e-05\n",
      "Epoch [129/200]  train_loss=0.00454  val_loss=0.00504  lr=1.00e-05\n",
      "Epoch [130/200]  train_loss=0.00453  val_loss=0.00501  lr=1.00e-05\n",
      "Epoch [131/200]  train_loss=0.00453  val_loss=0.00501  lr=1.00e-05\n",
      "Epoch [132/200]  train_loss=0.00453  val_loss=0.00500  lr=1.00e-05\n",
      "Epoch [133/200]  train_loss=0.00453  val_loss=0.00502  lr=1.00e-05\n",
      "Epoch [134/200]  train_loss=0.00453  val_loss=0.00504  lr=1.00e-05\n",
      "Epoch [135/200]  train_loss=0.00452  val_loss=0.00510  lr=1.00e-05\n",
      "Epoch [136/200]  train_loss=0.00456  val_loss=0.00499  lr=1.00e-05\n",
      "Epoch [137/200]  train_loss=0.00452  val_loss=0.00503  lr=1.00e-05\n",
      "Epoch [138/200]  train_loss=0.00452  val_loss=0.00504  lr=1.00e-05\n",
      "Epoch [139/200]  train_loss=0.00451  val_loss=0.00505  lr=1.00e-05\n",
      "Epoch [140/200]  train_loss=0.00452  val_loss=0.00500  lr=1.00e-05\n",
      "Epoch [141/200]  train_loss=0.00455  val_loss=0.00500  lr=1.00e-05\n",
      "Epoch [142/200]  train_loss=0.00453  val_loss=0.00498  lr=1.00e-05\n",
      "Epoch [143/200]  train_loss=0.00451  val_loss=0.00501  lr=1.00e-05\n",
      "Epoch [144/200]  train_loss=0.00451  val_loss=0.00499  lr=1.00e-05\n",
      "Epoch [145/200]  train_loss=0.00451  val_loss=0.00502  lr=1.00e-05\n",
      "Epoch [146/200]  train_loss=0.00450  val_loss=0.00497  lr=1.00e-05\n",
      "Epoch [147/200]  train_loss=0.00451  val_loss=0.00501  lr=1.00e-05\n",
      "Epoch [148/200]  train_loss=0.00450  val_loss=0.00497  lr=1.00e-05\n",
      "Epoch [149/200]  train_loss=0.00453  val_loss=0.00502  lr=1.00e-05\n",
      "Epoch [150/200]  train_loss=0.00450  val_loss=0.00503  lr=1.00e-06\n",
      "Epoch [151/200]  train_loss=0.00449  val_loss=0.00501  lr=1.00e-06\n",
      "Epoch [152/200]  train_loss=0.00449  val_loss=0.00500  lr=1.00e-06\n",
      "Epoch [153/200]  train_loss=0.00449  val_loss=0.00500  lr=1.00e-06\n",
      "Epoch [154/200]  train_loss=0.00449  val_loss=0.00499  lr=1.00e-06\n",
      "Epoch [155/200]  train_loss=0.00449  val_loss=0.00500  lr=1.00e-06\n",
      "Epoch [156/200]  train_loss=0.00449  val_loss=0.00500  lr=1.00e-06\n",
      "Epoch [157/200]  train_loss=0.00449  val_loss=0.00500  lr=1.00e-06\n",
      "Epoch [158/200]  train_loss=0.00449  val_loss=0.00499  lr=1.00e-06\n",
      "Epoch [159/200]  train_loss=0.00449  val_loss=0.00500  lr=1.00e-06\n",
      "Epoch [160/200]  train_loss=0.00449  val_loss=0.00500  lr=1.00e-06\n",
      "Epoch [161/200]  train_loss=0.00449  val_loss=0.00499  lr=1.00e-06\n",
      "Epoch [162/200]  train_loss=0.00449  val_loss=0.00500  lr=1.00e-06\n",
      "Epoch [163/200]  train_loss=0.00449  val_loss=0.00500  lr=1.00e-06\n",
      "Epoch [164/200]  train_loss=0.00449  val_loss=0.00499  lr=1.00e-06\n",
      "Epoch [165/200]  train_loss=0.00449  val_loss=0.00500  lr=1.00e-06\n",
      "Epoch [166/200]  train_loss=0.00449  val_loss=0.00499  lr=1.00e-06\n",
      "Epoch [167/200]  train_loss=0.00449  val_loss=0.00499  lr=1.00e-06\n",
      "Epoch [168/200]  train_loss=0.00449  val_loss=0.00499  lr=1.00e-06\n",
      "Epoch [169/200]  train_loss=0.00449  val_loss=0.00500  lr=1.00e-06\n",
      "Epoch [170/200]  train_loss=0.00449  val_loss=0.00500  lr=1.00e-06\n",
      "Epoch [171/200]  train_loss=0.00449  val_loss=0.00499  lr=1.00e-06\n",
      "Epoch [172/200]  train_loss=0.00449  val_loss=0.00500  lr=1.00e-06\n",
      "Epoch [173/200]  train_loss=0.00449  val_loss=0.00499  lr=1.00e-06\n",
      "Epoch [174/200]  train_loss=0.00449  val_loss=0.00499  lr=1.00e-06\n",
      "Epoch [175/200]  train_loss=0.00449  val_loss=0.00500  lr=1.00e-06\n",
      "Epoch [176/200]  train_loss=0.00449  val_loss=0.00500  lr=1.00e-06\n",
      "Epoch [177/200]  train_loss=0.00449  val_loss=0.00499  lr=1.00e-06\n",
      "Epoch [178/200]  train_loss=0.00449  val_loss=0.00499  lr=1.00e-06\n",
      "Epoch [179/200]  train_loss=0.00449  val_loss=0.00499  lr=1.00e-06\n",
      "Epoch [180/200]  train_loss=0.00449  val_loss=0.00500  lr=1.00e-06\n",
      "Epoch [181/200]  train_loss=0.00449  val_loss=0.00499  lr=1.00e-06\n",
      "Epoch [182/200]  train_loss=0.00449  val_loss=0.00500  lr=1.00e-06\n",
      "Epoch [183/200]  train_loss=0.00448  val_loss=0.00499  lr=1.00e-06\n",
      "Epoch [184/200]  train_loss=0.00449  val_loss=0.00498  lr=1.00e-06\n",
      "Epoch [185/200]  train_loss=0.00449  val_loss=0.00500  lr=1.00e-06\n",
      "Epoch [186/200]  train_loss=0.00448  val_loss=0.00499  lr=1.00e-06\n",
      "Epoch [187/200]  train_loss=0.00449  val_loss=0.00498  lr=1.00e-06\n",
      "Epoch [188/200]  train_loss=0.00449  val_loss=0.00499  lr=1.00e-06\n",
      "Epoch [189/200]  train_loss=0.00449  val_loss=0.00498  lr=1.00e-06\n",
      "Epoch [190/200]  train_loss=0.00449  val_loss=0.00499  lr=1.00e-06\n",
      "Epoch [191/200]  train_loss=0.00449  val_loss=0.00498  lr=1.00e-06\n",
      "Epoch [192/200]  train_loss=0.00449  val_loss=0.00500  lr=1.00e-06\n",
      "Epoch [193/200]  train_loss=0.00449  val_loss=0.00497  lr=1.00e-06\n",
      "Epoch [194/200]  train_loss=0.00448  val_loss=0.00499  lr=1.00e-06\n",
      "Epoch [195/200]  train_loss=0.00448  val_loss=0.00499  lr=1.00e-06\n",
      "Epoch [196/200]  train_loss=0.00448  val_loss=0.00500  lr=1.00e-06\n",
      "Epoch [197/200]  train_loss=0.00448  val_loss=0.00499  lr=1.00e-06\n",
      "Epoch [198/200]  train_loss=0.00448  val_loss=0.00499  lr=1.00e-06\n",
      "Epoch [199/200]  train_loss=0.00448  val_loss=0.00498  lr=1.00e-06\n",
      "Epoch [200/200]  train_loss=0.00448  val_loss=0.00498  lr=1.00e-07\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to CANN_mid4.torch, best val_loss=0.004965\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 1. \n",
    "# -----------------------------\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "mu = 384.614\n",
    "lamda = 576.923\n",
    "\n",
    "#  (I1, J) \n",
    "min_I1, max_I1 = 0.08, 20.0\n",
    "min_J,  max_J  = 0.04, 10.0\n",
    "\n",
    "#  ()\n",
    "n_samples = 30000  # 3\n",
    "\n",
    "batch_size = 1024\n",
    "hidden_dim = 128\n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "epochs = 200  #  epoch 200\n",
    "lr = 1e-3     # \n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 2.  () +  psi +  train/val\n",
    "# ------------------------------------------------------\n",
    "torch.manual_seed(42)  # \n",
    "I1_rand = torch.rand(n_samples)*(max_I1 - min_I1) + min_I1\n",
    "J_rand  = torch.rand(n_samples)*(max_J  - min_J)  + min_J\n",
    "\n",
    "# \n",
    "psi_rand = lamda/2 * (torch.log(J_rand))**2 - mu*torch.log(J_rand) + mu/2*(I1_rand - 2)\n",
    "\n",
    "# \n",
    "x_all = torch.stack([I1_rand, J_rand], dim=1)\n",
    "y_all = psi_rand.view(-1, 1)\n",
    "\n",
    "#  train:val=8:2\n",
    "train_size = int(0.8 * n_samples)\n",
    "val_size   = n_samples - train_size\n",
    "#  x_all,y_all permute\n",
    "dataset_all = TensorDataset(x_all, y_all)\n",
    "train_dataset, val_dataset = random_split(dataset_all, [train_size, val_size])\n",
    "\n",
    "#  train_mean,train_std ()\n",
    "train_x_ori = train_dataset[:][0]  #  x\n",
    "train_y_ori = train_dataset[:][1]\n",
    "\n",
    "train_x_mean = train_x_ori.mean(dim=0)\n",
    "train_x_std  = train_x_ori.std(dim=0)\n",
    "train_y_mean = train_y_ori.mean(dim=0)\n",
    "train_y_std  = train_y_ori.std(dim=0)\n",
    "\n",
    "# std0\n",
    "train_x_std[train_x_std==0] = 1e-8\n",
    "train_y_std[train_y_std==0] = 1e-8\n",
    "\n",
    "def normalize_x(x):\n",
    "    return (x - train_x_mean)/train_x_std\n",
    "def normalize_y(y):\n",
    "    return (y - train_y_mean)/train_y_std\n",
    "\n",
    "def denormalize_y(y_norm):\n",
    "    return y_norm*train_y_std + train_y_mean\n",
    "\n",
    "# train,val\n",
    "train_x_norm = normalize_x(train_x_ori)\n",
    "train_y_norm = normalize_y(train_y_ori)\n",
    "\n",
    "val_x_ori = val_dataset[:][0]\n",
    "val_y_ori = val_dataset[:][1]\n",
    "val_x_norm = normalize_x(val_x_ori)\n",
    "val_y_norm = normalize_y(val_y_ori)\n",
    "\n",
    "# DataLoader\n",
    "train_data = TensorDataset(train_x_norm, train_y_norm)\n",
    "val_data   = TensorDataset(val_x_norm, val_y_norm)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_data,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. MLP (SiLU)\n",
    "# ----------------------------------------\n",
    "## This class creates the actual Neural Network.\n",
    "class MLNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLNet, self).__init__()\n",
    "\n",
    "        # First layer: half linear, half quadratic\n",
    "        self.layer1_linear = nn.Linear(input_dim, hidden_dim // 2)\n",
    "        self.layer1_quadratic = nn.Linear(input_dim, hidden_dim // 2)\n",
    "\n",
    "        # Second layer: half exponential, half linear\n",
    "        self.layer2_linear = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.layer2_exponential = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "\n",
    "        # Final layer to combine outputs\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First layer transformations\n",
    "        linear_out = self.layer1_linear(x)\n",
    "        quadratic_out = torch.pow(self.layer1_quadratic(x), 2)\n",
    "        combined1 = torch.cat((linear_out, quadratic_out), dim=1)\n",
    "\n",
    "        # Second layer transformations\n",
    "        linear_out2 = self.layer2_linear(combined1)\n",
    "        exp_out = torch.exp(self.layer2_exponential(combined1))\n",
    "        combined2 = torch.cat((linear_out2, exp_out), dim=1)\n",
    "\n",
    "        # Final output layer\n",
    "        output = self.output_layer(combined2)\n",
    "        return output\n",
    "\n",
    "# -----------------------------\n",
    "# 4.  +  + \n",
    "# -----------------------------\n",
    "def train_model(epochs=epochs, lr=lr):\n",
    "    model = MLNet(input_dim, hidden_dim, output_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    #  StepLR50 epochlr0.1\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    best_val_loss = 1e9\n",
    "    best_model_state = None\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history   = []\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        #  epoch\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for bx, by in train_loader:\n",
    "            bx = bx.to(device)\n",
    "            by = by.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(bx)\n",
    "            loss = criterion(out, by)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()*bx.size(0)  # \n",
    "\n",
    "        #  train_loss\n",
    "        train_loss = running_loss/len(train_loader.dataset)\n",
    "        train_loss_history.append(train_loss)\n",
    "\n",
    "        # \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_running_loss = 0.0\n",
    "            for vx, vy in val_loader:\n",
    "                vx = vx.to(device)\n",
    "                vy = vy.to(device)\n",
    "                vout = model(vx)\n",
    "                vloss = criterion(vout, vy)\n",
    "                val_running_loss += vloss.item()*vx.size(0)\n",
    "            val_loss = val_running_loss/len(val_loader.dataset)\n",
    "            val_loss_history.append(val_loss)\n",
    "\n",
    "        # \n",
    "        scheduler.step()\n",
    "\n",
    "        # Early Stopping/\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        print(f\"Epoch [{ep+1}/{epochs}]  train_loss={train_loss:.5f}  val_loss={val_loss:.5f}  lr={scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "    # \n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # /\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(train_loss_history, label='train_loss')\n",
    "    plt.plot(val_loss_history,   label='val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    model_file = \"CANN_mid4.torch\"\n",
    "    # \n",
    "    torch.save({\n",
    "        'model_state': model.state_dict(),\n",
    "        'x_mean': train_x_mean,\n",
    "        'x_std' : train_x_std,\n",
    "        'y_mean': train_y_mean,\n",
    "        'y_std' : train_y_std\n",
    "    }, model_file)\n",
    "    print(f\"Model saved to {model_file}, best val_loss={best_val_loss:.6f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = train_model(epochs=epochs, lr=lr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
