{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'CANN_mid4.torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 102\u001b[0m\n\u001b[1;32m    100\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(combined2)\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m--> 102\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCANN_mid4.torch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgeteps\u001b[39m(X, U):\n\u001b[1;32m    106\u001b[0m     duxdxy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(U[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), X, torch\u001b[38;5;241m.\u001b[39mones(X\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice),\n\u001b[1;32m    107\u001b[0m                                  create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CANN_mid4.torch'"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams[\"text.usetex\"] = True\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.set_num_threads(8) # Use _maximally_ 8 CPU cores\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "#device = torch.device(\"cpu\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "device = torch.device(device)\n",
    "\n",
    "\n",
    "\n",
    "Lx = 2\n",
    "Ly = 1\n",
    "samples_x = 20 * Lx# + 1\n",
    "samples_y = 20 * Ly# + 1\n",
    "delta_x = Lx / (samples_x)# - 1)\n",
    "delta_y = Ly / (samples_y)# - 1)\n",
    "\n",
    "disp_left = 0\n",
    "force_right = 0.2\n",
    "force_upper = 0\n",
    "force_lower = 0\n",
    "\n",
    "\"\"\"\n",
    "/|-----------------------| ->\n",
    "/|                       | ->\n",
    "/|                       | ->\n",
    "/|-----------------------| ->\n",
    "\"\"\"\n",
    "\n",
    "sample_points = torch.meshgrid(torch.linspace(0, Lx, samples_x), torch.linspace(0, Ly, samples_y), indexing='ij')\n",
    "sample_points = torch.cat((sample_points[0].reshape(-1, 1), sample_points[1].reshape(-1, 1)), dim=1)\n",
    "sample_points = sample_points.to(device)\n",
    "sample_points.requires_grad_(True)\n",
    "\n",
    "\n",
    "hidden_dim = 64\n",
    "input_dim = 2\n",
    "output_dim = 2\n",
    "\n",
    "\n",
    "def plot_undef ():\n",
    "    plt.subplot(1,1,1)\n",
    "    plt.scatter(sample_points[:, 0].detach().numpy(), sample_points[:, 1].detach().numpy(), c='black', s=5)\n",
    "\n",
    "E = 1\n",
    "nu = 0.3\n",
    "\n",
    "# convert to plane strain\n",
    "E = E/(1 - nu**2)\n",
    "nu = nu / (1 - nu)\n",
    "\n",
    "\n",
    "mu = E/(2*(1+nu))\n",
    "lam = E*nu / ((1+nu)*(1-2*nu))\n",
    "\n",
    "\n",
    "# normalization parameter\n",
    "train_x_mean= torch.tensor([15.0400,  1.0200]).to(device)\n",
    "train_x_std= torch.tensor([8.6458, 0.5664]).to(device)\n",
    "train_y_mean= 3.4464268684387207\n",
    "train_y_std= 2.6058945655822754\n",
    "\n",
    "\n",
    "# load trained CANN model\n",
    "class MLNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLNet, self).__init__()\n",
    "\n",
    "        # First layer: half linear, half quadratic\n",
    "        self.layer1_linear = nn.Linear(input_dim, hidden_dim // 2)\n",
    "        self.layer1_quadratic = nn.Linear(input_dim, hidden_dim // 2)\n",
    "\n",
    "        # Second layer: half exponential, half linear\n",
    "        self.layer2_linear = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.layer2_exponential = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "\n",
    "        # Final layer to combine outputs\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First layer transformations\n",
    "        linear_out = self.layer1_linear(x)\n",
    "        quadratic_out = torch.pow(self.layer1_quadratic(x), 2)\n",
    "        combined1 = torch.cat((linear_out, quadratic_out), dim=1)\n",
    "\n",
    "        # Second layer transformations\n",
    "        linear_out2 = self.layer2_linear(combined1)\n",
    "        exp_out = torch.exp(self.layer2_exponential(combined1))\n",
    "        combined2 = torch.cat((linear_out2, exp_out), dim=1)\n",
    "\n",
    "        # Final output layer\n",
    "        output = self.output_layer(combined2)\n",
    "        return output\n",
    "model = torch.load('CANN_mid4.torch', weights_only=False)\n",
    "\n",
    "\n",
    "def geteps(X, U):\n",
    "    duxdxy = torch.autograd.grad(U[:, 0].unsqueeze(1), X, torch.ones(X.size()[0], 1, device=device),\n",
    "                                 create_graph=True, retain_graph=True)[0]\n",
    "    duydxy = torch.autograd.grad(U[:, 1].unsqueeze(1), X, torch.ones(X.size()[0], 1, device=device),\n",
    "                                 create_graph=True, retain_graph=True)[0]\n",
    "    H = torch.zeros(X.size()[0], X.size()[1], X.size()[1], device=device)\n",
    "    H[:, 0, :] = duxdxy\n",
    "    H[:, 1, :] = duydxy\n",
    "    H = H.reshape(samples_x, samples_y, 2, 2)\n",
    "    eps = H\n",
    "    eps[:, :, [0, 1], [1, 0]] = 0.5 * (eps[:, :, 0, 1] + eps[:, :, 1, 0]).unsqueeze(2).expand(samples_x, samples_y, 2)\n",
    "    return eps\n",
    "\n",
    "def material_model(eps):\n",
    "    tr_eps = eps.diagonal(offset=0, dim1=-1, dim2=-2).sum(-1)\n",
    "    sig = 2 * mu * eps + lam * torch.einsum('ij,kl->ijkl', tr_eps, torch.eye(eps.size()[-1], device=device))\n",
    "    psi = torch.einsum('ijkl,ijkl->ij', eps, sig)\n",
    "    return psi, sig\n",
    "\n",
    "def NeoHookean2D(X, U):\n",
    "    duxdxy = torch.autograd.grad(U[:, 0].unsqueeze(1), X, torch.ones(X.size()[0], 1, device=device), create_graph=True, retain_graph=True)[0]\n",
    "    duydxy = torch.autograd.grad(U[:, 1].unsqueeze(1), X, torch.ones(X.size()[0], 1, device=device), create_graph=True, retain_graph=True)[0]\n",
    "    Fxx = duxdxy[:, 0].unsqueeze(1) + 1\n",
    "    Fxy = duxdxy[:, 1].unsqueeze(1) + 0\n",
    "    Fyx = duydxy[:, 0].unsqueeze(1) + 0\n",
    "    Fyy = duydxy[:, 1].unsqueeze(1) + 1\n",
    "    detF = Fxx * Fyy - Fxy * Fyx\n",
    "    trC = Fxx ** 2 + Fxy ** 2 + Fyx ** 2 + Fyy ** 2\n",
    "    #psi = 0.5 * lam * (torch.log(detF) * torch.log(detF)) - mu * torch.log(detF) + 0.5 * mu * (trC - 2)\n",
    "\n",
    "    # calculate psi using CANN model\n",
    "    \n",
    "    # inputs are trC and detF\n",
    "    test_x = torch.cat((trC, detF), dim=1)\n",
    "    # normalize input\n",
    "    test_x = (test_x - train_x_mean) / train_x_std\n",
    "\n",
    "\n",
    "    model.eval() ## Set the NN model into evaluation mode\n",
    "    out = model(test_x.to(device)) ## Call the model, i.e. perform the actual inference\n",
    "\n",
    "    psi = out * train_y_std + train_y_mean\n",
    "    \n",
    "\n",
    "    # Calculate Piola Kirchoff\n",
    "\n",
    "    # Compute Piola-Kirchhoff stress components\n",
    "    Pxx = torch.autograd.grad(psi, Fxx, grad_outputs=torch.ones_like(psi), create_graph=True)[0]  # ∂ψ / ∂Fxx\n",
    "    Pxy = torch.autograd.grad(psi, Fxy, grad_outputs=torch.ones_like(psi), create_graph=True)[0]  # ∂ψ / ∂Fxy\n",
    "    Pyx = torch.autograd.grad(psi, Fyx, grad_outputs=torch.ones_like(psi), create_graph=True)[0]  # ∂ψ / ∂Fyx\n",
    "    Pyy = torch.autograd.grad(psi, Fyy, grad_outputs=torch.ones_like(psi), create_graph=True)[0]  # ∂ψ / ∂Fyy\n",
    "\n",
    "    # Reshape components to (samples_x, samples_y)\n",
    "    # Assuming that the number of grid points N = samples_x * samples_y\n",
    "    Pxx = Pxx.view(samples_x, samples_y)\n",
    "    Pxy = Pxy.view(samples_x, samples_y)\n",
    "    Pyx = Pyx.view(samples_x, samples_y)\n",
    "    Pyy = Pyy.view(samples_x, samples_y)\n",
    "\n",
    "    # Stack components to form 2x2 matrix for each grid point\n",
    "    P = torch.stack([\n",
    "        torch.stack([Pxx, Pxy], dim=-1),  # First row: Pxx, Pxy\n",
    "        torch.stack([Pyx, Pyy], dim=-1)   # Second row: Pyx, Pyy\n",
    "    ], dim=-2)  # Stack the rows to create a 2x2 matrix , shape: (samples_x, samples_y, 2, 2)\n",
    "\n",
    "    # Compute Cauchy stress tensor sigma\n",
    "    Fxx = Fxx.view(samples_x, samples_y)\n",
    "    Fxy = Fxy.view(samples_x, samples_y)\n",
    "    Fyx = Fyx.view(samples_x, samples_y)\n",
    "    Fyy = Fyy.view(samples_x, samples_y)\n",
    "\n",
    "    FinvT = torch.inverse(torch.stack([\n",
    "        torch.stack([Fxx, Fxy], dim=-1),  # First row: Fxx, Fxy\n",
    "        torch.stack([Fyx, Fyy], dim=-1)   # Second row: Fyx, Fyy\n",
    "    ], dim=-2)).transpose(-1, -2)  # Transpose for F^-T\n",
    "    \n",
    "    sigma = (1 / detF.view(samples_x, samples_y, 1, 1)) * torch.matmul(P, FinvT)\n",
    "    \n",
    "    return psi, P, sigma\n",
    "\n",
    "\n",
    "def integratePsi (psi):\n",
    "    # PyTorch has a built-in function for trapezoidal integration.\n",
    "    # Hence, we can use it (nested for the 2 dimensions)\n",
    "    # The comparison with midpoint rule showed no\n",
    "    # difference to the midpoint rule significant for the basic stability of this DEM code\n",
    "    #PSI = torch.trapezoid(torch.trapezoid(psi, dx = delta_y, dim=1), dx = delta_x, dim=0)\n",
    "\n",
    "    # Apply Simpson's rule along the y-axis (dim=1)\n",
    "    n_y = psi.shape[1]\n",
    "    weights_y = torch.ones(n_y).to(device)\n",
    "    weights_y[1:-1:2] = 4\n",
    "    weights_y[2:-1:2] = 2\n",
    "    integral_y = (delta_y / 3) * torch.sum(weights_y * psi, dim=1)\n",
    "\n",
    "    # Apply Simpson's rule along the x-axis (dim=0) to the result of the y-axis integration\n",
    "    n_x = psi.shape[0]\n",
    "    weights_x = torch.ones(n_x).to(device)\n",
    "    weights_x[1:-1:2] = 4\n",
    "    weights_x[2:-1:2] = 2\n",
    "    PSI = (delta_x / 3) * torch.sum(weights_x * integral_y)\n",
    "\n",
    "    return PSI\n",
    "\n",
    "def calculateDivergenceLoss(sig):\n",
    "    criterion = nn.MSELoss(reduction=\"sum\")\n",
    "    div_sig = (1 / (2 * delta_x) * (sig[2:, 1:-1, :, 0] - sig[:-2, 1:-1, :, 0]))\n",
    "    div_sig += (1 / (2 * delta_y) * (sig[1:-1, 2:, :, 1] - sig[1:-1, :-2, :, 1]))\n",
    "    div_loss = criterion(div_sig, torch.zeros(samples_x - 2, samples_y - 2, 2, device=device))\n",
    "    print(\"div_loss: \", div_loss)\n",
    "    return div_loss\n",
    "\n",
    "def calculateDivergenceLossP(P):\n",
    "    criterion = nn.MSELoss(reduction=\"sum\")\n",
    "    div_P = (1 / (2 * delta_x) * (P[2:, 1:-1, 0, 0] - P[:-2, 1:-1, 0, 0]))\n",
    "    div_P += (1 / (2 * delta_y) * (P[1:-1, 2:, 1, 1] - P[1:-1, :-2, 1, 1]))\n",
    "    div_loss = criterion(div_P, torch.zeros(samples_x - 2, samples_y - 2, device=device))\n",
    "    print(\"div_loss: \", div_loss)\n",
    "    return div_loss\n",
    "def calculateDivergenceLossP(P):\n",
    "    criterion = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "    # Calculate divergence in x-direction (d/dx) for Pxx and Pxy components\n",
    "    div_P_x = (1 / (2 * delta_x) * (P[2:, 1:-1, 0, 0] - P[:-2, 1:-1, 0, 0]))  # ∂Pxx/∂x\n",
    "    div_P_x += (1 / (2 * delta_y) * (P[1:-1, 2:, 0, 1] - P[1:-1, :-2, 0, 1]))  # ∂Pxy/∂y\n",
    "\n",
    "    # Calculate divergence in y-direction (d/dy) for Pyx and Pyy components\n",
    "    div_P_y = (1 / (2 * delta_x) * (P[2:, 1:-1, 1, 0] - P[:-2, 1:-1, 1, 0]))  # ∂Pyx/∂x\n",
    "    div_P_y += (1 / (2 * delta_y) * (P[1:-1, 2:, 1, 1] - P[1:-1, :-2, 1, 1]))  # ∂Pyy/∂y\n",
    "\n",
    "    # Combine the x and y components of the divergence\n",
    "    div_P = div_P_x + div_P_y\n",
    "\n",
    "    # Compute the divergence loss with respect to zero (as per the weak form)\n",
    "    div_loss = criterion(div_P, torch.zeros(samples_x - 2, samples_y - 2, device=device))\n",
    "\n",
    "    print(\"div_loss: \", div_loss)\n",
    "    return div_loss\n",
    "\n",
    "\n",
    "def integrateTractionEnergy(U, sig):\n",
    "    # Again, use built-in integration\n",
    "    # Before, there was a bug: Only the farmost right points have to be integrated\n",
    "    # Not U[:, :, 0], but U[-1, :, 0]\n",
    "    # This bug is fixed here:\n",
    "    #Ty = force_right * torch.trapezoid(U[-1, :, 0], dx = delta_y)\n",
    "\n",
    "    # Extract the displacement at the right boundary\n",
    "    U_right = U[-1, :, 0]\n",
    "\n",
    "    # Number of points (make sure to have an even number of points for Simpson's rule)\n",
    "    n = U_right.size(0)\n",
    "    \n",
    "    if n % 2 == 1:\n",
    "        raise ValueError(\"Number of points must be even for Simpson's rule.\")\n",
    "\n",
    "    # Apply Simpson's rule\n",
    "    weights = torch.ones(n, device=device)\n",
    "    weights[1:-1:2] = 4  # Multiply every other weight by 4 (odd indices)\n",
    "    weights[2:-1:2] = 2  # Multiply intermediate weights by 2 (even indices)\n",
    "\n",
    "    # Calculate the integral using Simpson's rule\n",
    "    integral = (delta_y / 3) * torch.sum(weights * U_right)\n",
    "    # Then calculate Ty\n",
    "    Ty = force_right * integral\n",
    "\n",
    "    # Theoretically, the traction is calculated from the stress, not the force boundary condition\n",
    "    # So it's up to comparison what performs better.\n",
    "    #Ty = torch.trapezoid(sig[-1, :, 0, 0] * U[-1, :, 0], dx=delta_y)\n",
    "\n",
    "    # The x-component was not necessary, so far\n",
    "    # Careful: the bug was not fixed here!\n",
    "    #Tx = (sig[1:-1, :, 0, 0] * U[1:-1, :, 0]).sum()\n",
    "    #Tx += (sig[:, 1:-1, 1, 1] * U[:, 1:-1, 1]).sum()\n",
    "    #Tx += 0.5 * (sig[[0, -1], [0, -1], 1, 1] * U[[0, -1], [0, -1], 1]).sum()\n",
    "    #Tx *= delta_x\n",
    "\n",
    "    return Ty #+ Tx\n",
    "\n",
    "def boundaryLosses(U, sig):\n",
    "    criterion = nn.MSELoss(reduction=\"sum\")\n",
    "    # New factor to multiply the losses for fixed / Dirichlet B.C.\n",
    "    # The factor 100*E was too large, i.e. the other conditions were not respected in comparison.\n",
    "    # The factor 1*E was too small, i.e. the B.C. was not fulfilled well\n",
    "    loss_left = criterion(delta_y * U[0, :, :] * E*10, torch.zeros(samples_y, 2, device=device))\n",
    "\n",
    "    # no fundamental changes\n",
    "    forces_right = torch.ones(samples_y, device=device)\n",
    "    forces_right[[0, -1]] = 0.5 * torch.ones(2, device=device)\n",
    "    forces_right *= force_right * delta_y\n",
    "\n",
    "    # Factor of 10 introduced to improve the compliance with this B.C.\n",
    "    tractions_right = sig[-1, :, 0, 0] * delta_y\n",
    "    tractions_right[[0, -1]] *= 0.5\n",
    "    loss_right = 10*criterion(tractions_right, forces_right)\n",
    "\n",
    "    # no changes\n",
    "    sheartractions_right = sig[-1, :, 0, 1] * delta_y\n",
    "    loss_right += criterion(sheartractions_right, torch.zeros(samples_y, device=device))\n",
    "    tractions_lower = sig[1:-1, 0, 1, 1] * delta_x\n",
    "    loss_lower = criterion(tractions_lower, torch.zeros(samples_x - 2, device=device))\n",
    "    sheartractions_lower = sig[1:-1, 0, 1, 0] * delta_x\n",
    "    loss_lower += criterion(sheartractions_lower, torch.zeros(samples_x - 2, device=device))\n",
    "    tractions_upper = sig[1:-1, -1, 1, 1] * delta_x\n",
    "    loss_upper = criterion(tractions_upper, torch.zeros(samples_x - 2, device=device))\n",
    "    sheartractions_upper = sig[1:-1, -1, 1, 0] * delta_x\n",
    "    loss_upper += criterion(sheartractions_upper, torch.zeros(samples_x - 2, device=device))\n",
    "\n",
    "    print(\"loss_left: \", loss_left)\n",
    "    print(\"loss_right: \", loss_right)\n",
    "\n",
    "    bc_losses = loss_right + loss_left + loss_upper + loss_lower\n",
    "    return bc_losses\n",
    "\n",
    "def losses(U, plot, data_driven):\n",
    "    eps = geteps(sample_points, U)\n",
    "    # psi, sig = material_model(eps)\n",
    "    psi, P, sig = NeoHookean2D(sample_points, U)\n",
    "\n",
    "    # Cool plots to better understand what's going on\n",
    "    if plot:\n",
    "        plt.figure(figsize=(25, 25))\n",
    "        plt.subplot(7, 3, 1)\n",
    "        plt.gca().set_aspect('equal')\n",
    "        plt.scatter((sample_points[:, 0] + U[:, 0]).detach().cpu().numpy(),\n",
    "                    (sample_points[:, 1] + U[:, 1]).detach().cpu().numpy(),\n",
    "                    c=(eps[:, :, 0, 0].detach().cpu().numpy()))\n",
    "        plt.colorbar()\n",
    "        plt.title(\"eps_x\")\n",
    "        plt.subplot(7, 3, 2)\n",
    "        plt.gca().set_aspect('equal')\n",
    "        plt.scatter((sample_points[:, 0] + U[:, 0]).detach().cpu().numpy(),\n",
    "                    (sample_points[:, 1] + U[:, 1]).detach().cpu().numpy(),\n",
    "                    c=(eps[:, :, 1, 1].detach().cpu().numpy()))\n",
    "        plt.colorbar()\n",
    "        plt.title(\"eps_y\")\n",
    "        plt.subplot(7, 3, 3)\n",
    "        plt.gca().set_aspect('equal')\n",
    "        plt.scatter((sample_points[:, 0] + U[:, 0]).detach().cpu().numpy(),\n",
    "                    (sample_points[:, 1] + U[:, 1]).detach().cpu().numpy(),\n",
    "                    c=(eps[:, :, 0, 1].detach().cpu().numpy()))\n",
    "        plt.title(\"eps_xy\")\n",
    "        plt.colorbar()\n",
    "        plt.subplot(7, 3, 4)\n",
    "        plt.gca().set_aspect('equal')\n",
    "        plt.scatter((sample_points[:, 0] + U[:, 0]).detach().cpu().numpy(),\n",
    "                    (sample_points[:, 1] + U[:, 1]).detach().cpu().numpy(),\n",
    "                    c=(psi[:, :].detach().cpu().numpy()))\n",
    "        plt.colorbar()\n",
    "        plt.title(\"psi\")\n",
    "\n",
    "        plt.subplot(7, 3, 7)\n",
    "        plt.gca().set_aspect('equal')\n",
    "        plt.scatter((sample_points[:, 0] + U[:, 0]).detach().cpu().numpy(),\n",
    "                    (sample_points[:, 1] + U[:, 1]).detach().cpu().numpy(),\n",
    "                    c=(U[:, 0].detach().cpu().numpy()))\n",
    "        plt.title(\"U_x\")\n",
    "        plt.colorbar()\n",
    "        plt.subplot(7, 3, 8)\n",
    "        plt.gca().set_aspect('equal')\n",
    "        plt.scatter((sample_points[:, 0] + U[:, 0]).detach().cpu().numpy(),\n",
    "                    (sample_points[:, 1] + U[:, 1]).detach().cpu().numpy(),\n",
    "                    c=(U[:, 1].detach().cpu().numpy()))\n",
    "        plt.title(\"U_y\")\n",
    "        plt.colorbar()\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "\n",
    "    U = U.reshape(samples_x, samples_y, 2)\n",
    "    PSI = integratePsi(psi)\n",
    "    T = integrateTractionEnergy(U, sig)\n",
    "    bc_losses = boundaryLosses(U, sig)\n",
    "    # The divergence loss result is not used anymore,\n",
    "    # but calculated and printed for more insight into the behavior/ what's going on\n",
    "\n",
    "    div_losses = calculateDivergenceLoss(P)\n",
    "    # print(\"T-PSI: \", T - PSI)\n",
    "    # print(\"PSI: \", PSI)\n",
    "    # print(\"T: \", T)\n",
    "\n",
    "    # New: calculate an _approximated_ analytical solution\n",
    "    # (Actually, the lateral shrinkage is not linear!)\n",
    "    # It can be used for a data-driven training of the NN --> see, if the NN architecture\n",
    "    # is qualitatively capable of approximating the solution.\n",
    "    # It also helped to find bugs if the NN was trained data-driven first and then physics-informed, afterwards.\n",
    "    delta_l = force_right/(E*1/Lx)\n",
    "    U_target = torch.zeros_like(sample_points)\n",
    "    U_target[:, 0] = sample_points[:, 0]/Lx * delta_l\n",
    "    U_target[:, 1] = -sample_points[:, 0]/Lx * nu * (sample_points[:, 1] / Ly - 0.5) * delta_l\n",
    "\n",
    "    # Plot the _approximated solution_\n",
    "    if plot:\n",
    "        plt.subplot(7, 3, 10)\n",
    "        plt.gca().set_aspect('equal')\n",
    "        plt.scatter((sample_points[:, 0] + U_target[:, 0]).detach().cpu().numpy(),\n",
    "                    (sample_points[:, 1] + U_target[:, 1]).detach().cpu().numpy(),\n",
    "                    c='black')\n",
    "        plt.title(\"deformed\")\n",
    "\n",
    "        eps = geteps(sample_points, U_target)\n",
    "\n",
    "        plt.subplot(7, 3, 13)\n",
    "        plt.gca().set_aspect('equal')\n",
    "        plt.scatter((sample_points[:, 0] + U_target[:, 0]).detach().cpu().numpy(),\n",
    "                    (sample_points[:, 1] + U_target[:, 1]).detach().cpu().numpy(),\n",
    "                    c=(eps[:, :, 0, 0].detach().cpu().numpy()))\n",
    "        plt.colorbar()\n",
    "        plt.title(\"eps_x\")\n",
    "\n",
    "        plt.subplot(7, 3, 14)\n",
    "        plt.gca().set_aspect('equal')\n",
    "        plt.scatter((sample_points[:, 0] + U_target[:, 0]).detach().cpu().numpy(),\n",
    "                    (sample_points[:, 1] + U_target[:, 1]).detach().cpu().numpy(),\n",
    "                    c=(eps[:, :, 1, 1].detach().cpu().numpy()))\n",
    "        plt.colorbar()\n",
    "        plt.title(\"eps_y\")\n",
    "\n",
    "        plt.subplot(7, 3, 15)\n",
    "        plt.gca().set_aspect('equal')\n",
    "        plt.scatter((sample_points[:, 0] + U_target[:, 0]).detach().cpu().numpy(),\n",
    "                    (sample_points[:, 1] + U_target[:, 1]).detach().cpu().numpy(),\n",
    "                    c=(eps[:, :, 0, 1].detach().cpu().numpy()))\n",
    "        plt.colorbar()\n",
    "        plt.title(\"eps_xy\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # For data-driven training\n",
    "    U_target = U_target.reshape(samples_x, samples_y, 2)\n",
    "    criterion = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "    if data_driven:\n",
    "        print (\"data_driven\")\n",
    "        return criterion(U, U_target)\n",
    "    else:\n",
    "        #return torch.abs(T - PSI) + 1 * bc_losses  + 0 * div_losses\n",
    "        return PSI - T + 1 * bc_losses + div_losses\n",
    "\n",
    "\n",
    "\n",
    "# This allows to choose another (random) initialization for the NN parameters\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.uniform_(m.weight, 0, 0.01)\n",
    "        #torch.nn.init.normal_(m.weight, mean=0.5/hidden_dim, std=10*1.0/hidden_dim)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "# This is a custom activation function, that even has an own NN parameter (self.slope)\n",
    "class myPow(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.slope = torch.nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.pow(x, 2) + self.slope * x\n",
    "\n",
    "class MLDEMNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3):\n",
    "        super(MLDEMNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Construct the hidden layers dynamically based on num_layers\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim, bias=True))\n",
    "        #layers.append(nn.Tanh())  # Activation function\n",
    "        layers.append(myPow())  # Custom activation function\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim, bias=True))\n",
    "            #layers.append(nn.Tanh())\n",
    "            layers.append(myPow())\n",
    "\n",
    "        # Add the final layer\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim, bias=True))\n",
    "        \n",
    "        # Sequential model\n",
    "        self.fcnn1 = nn.Sequential(*layers)\n",
    "\n",
    "        # self.fcnn1 = nn.Sequential(\n",
    "        #     nn.Linear(input_dim, hidden_dim, bias=True),\n",
    "        #     nn.Tanh(),\n",
    "        #     #myPow(), #--> so easy is it to use a custom activation function\n",
    "        #     # Which one is better? Try it out!\n",
    "        #     nn.Linear(hidden_dim, output_dim, bias=True),\n",
    "        # )\n",
    "        self.fcnn1.apply(weights_init) #--> comment in to apply the custom (random) intialization function\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x_in = torch.cat((x[:, 0].unsqueeze(1)/Lx, x[:, 1].unsqueeze(1)/Ly - 0.5), dim=1) -> Normalize inputs, try it out!\n",
    "        x_in = x\n",
    "        out = force_right/(E*1/Lx) * self.fcnn1(x_in)\n",
    "        #out = self.fcnn1(x)\n",
    "        out[:, 0] *= x[:, 0]\n",
    "        out[:, 1] *= x[:, 0] #--> In the literature a common output transformation, not proven to be necessary, here\n",
    "        # In fact, that includes problem specific information and obstructs generalizability of the method\n",
    "        # to a certain extent\n",
    "        return out\n",
    "\n",
    "# Should the training start data-driven with the _approximated_ solution as target values?\n",
    "data_driven = False #True\n",
    "\n",
    "def train():\n",
    "    model = MLDEMNet(input_dim, hidden_dim, output_dim)\n",
    "    model.to(device)\n",
    "    #optimizer = torch.optim.LBFGS(model.parameters(), lr=0.01, max_iter=20)#, line_search_fn='strong_wolfe')\n",
    "    # Adam has given much better results than LBFGS during the test runs for this sourcecode version.\n",
    "    # lr=0.01 is not optimized, but this order of magnitude performed comparatively well.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)#, weight_decay=1e-4)\n",
    "    epochs = 20001\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        #if (epoch + 1) % 3 == 0:\n",
    "        #    global force_right\n",
    "        #    if force_right < 0.05:\n",
    "        #        force_right += 0.03\n",
    "        if epoch > 1000:\n",
    "            global data_driven\n",
    "            data_driven = False # --> the following epochs will be a kind of transfer learning\n",
    "        if (epoch) % 2000 == 0:\n",
    "            U = model(sample_points)\n",
    "            print(\"force_right: \", force_right)\n",
    "            losses(U, True, True)\n",
    "            plt.show()\n",
    "\n",
    "        def closure():\n",
    "            U = model(sample_points)\n",
    "            global data_driven\n",
    "            loss = losses(U, False, data_driven)\n",
    "            print('Epoch %i/%i, Total Loss: %.64e' % (epoch+1, epochs, loss.item()))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        loss = closure()\n",
    "        optimizer.step()\n",
    "        #optimizer.step(closure)\n",
    "    return\n",
    "\n",
    "train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
